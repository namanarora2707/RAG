{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84ca455",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a30520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "749c16c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pdf files to process\n",
      "Processing file: APAR 2024-25.pdf\n",
      "Loaded 5 pages\n",
      "Processing file: sdl pbl naman.pdf\n",
      "Loaded 8 pages\n",
      "Processing file: WORK AND CONDUCT REPORT.pdf\n",
      "Loaded 2 pages\n",
      "Total documents loaded: 15\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} pdf files to process\" )\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader=PyPDFLoader(str(pdf_file))\n",
    "            documents=loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file']=pdf_file.name\n",
    "                doc.metadata['file_type']='pdf'\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {pdf_file.name}: {e}\")\n",
    "    print(f\"Total documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents=process_all_pdfs(\"../data/text_files/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa1dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    if split_docs:\n",
    "        print(f\"Example chunk content:\")\n",
    "        print(f\"Example chunk text: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"Example chunk metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a90d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 15 documents into 33 chunks\n",
      "Example chunk content:\n",
      "Example chunk text: Annual Work Report of PTVT \n",
      "Period 2024-25 \n",
      "GENERAL INFORMAION \n",
      "Name in full LAKSHMI MATANI \n",
      "School ID 1309127 \n",
      "Husband’s Name LALIT ARORA \n",
      "Designation PART TIME VOCATIONAL TEACHER (PTVT) \n",
      "Subject  TY\n",
      "Example chunk metadata: {'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='Annual Work Report of PTVT \\nPeriod 2024-25 \\nGENERAL INFORMAION \\nName in full LAKSHMI MATANI \\nSchool ID 1309127 \\nHusband’s Name LALIT ARORA \\nDesignation PART TIME VOCATIONAL TEACHER (PTVT) \\nSubject  TYPOGRAPGHY & COMPUTER APPLICATIOM \\nDate of Birth 02.07.1975 \\nQualification M.A (Eng.), B.Ed. \\nOther Qualification 2 year diploma in stenography from SCVT, Diploma in computer \\nteacher training from LBSTI \\nDate of Initial Appointment 03.02.2001 \\nDate of Posting in the present School 27.04.2023 \\nResidential Address WP-38A, LIG FLATS, MAURYA ENCLAVE PITAMPURA DELHI- \\n110034 \\n \\nTotal Period allotted in a week The total period allotted in a \\nweek for teaching Vocational \\nSubjects only \\nThe total period allotted in a \\nweek for teaching other \\nsubjects only \\n36 36 NIL \\n \\nResult of Current Academic Session \\n \\nResult \\n Subject Classes \\ntaught \\nNo. of \\nStudents \\nappeared \\nNo. of \\nStudents \\npassed \\nResult \\n%age \\nQuality \\nIndex \\nNo. of \\nDistinctions \\n \\n INFORMATION \\nTECHNOLOGY'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='Result \\n Subject Classes \\ntaught \\nNo. of \\nStudents \\nappeared \\nNo. of \\nStudents \\npassed \\nResult \\n%age \\nQuality \\nIndex \\nNo. of \\nDistinctions \\n \\n INFORMATION \\nTECHNOLOGY \\nIX 156 156 100% 61.81 23  \\n INFORMATION \\nTECHNOLOGY \\nX 120 120 100% 75.67 62  \\n TYPOGRAPHY & \\nCA \\nXI 42 42 100% 77.23 27  \\n TYPOGRAPHY & \\nCA \\nXII 51 51 100% 83.53 46'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='Comparison of Board’s Result \\n \\nClass Subjects taught in \\nClasses \\nPrevious year’s \\nResult \\nCurrent Year’s \\nResult \\nDeviation in \\nresult (+) (-) \\nX INFORMATION \\nTECHNOLOGY \\n100% 100% NIL \\nXII TYPOGRAPHY & CA - 100% NIL \\n \\nSelf Assessment by the teacher \\nTeachers Diary written regularly Yes \\n1. CURRICULAR ACTIVITIES: \\nI Classroom Teaching : \\na. Whether term-wise syllabus were prepared at the beginning of the session Yes \\nb. Syllabus covered Yes \\nc. Portions not covered, reasons thereof No \\nII Lesson Planning: \\na. Whether Model lesson planning is indicated in a diary or not Yes \\nb. Whether Teaching Aids were used? Give details: Real Objects, Black Board, Chalk, \\nComputer/Laptop, Charts, Files. \\nc. Whether Teaching Aids prepared? Give details: Yes, Charts \\nIII Home Assignments: \\na. How many assignments on average are given per week? \\nTYPOGRAPHY & CA: Twice a week \\nINFORMATION TECHNOLOGY: Twice a week \\nb. What is the system of checking the notebooks? \\nTYPOGRAPHY & CA: Once a week'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='TYPOGRAPHY & CA: Twice a week \\nINFORMATION TECHNOLOGY: Twice a week \\nb. What is the system of checking the notebooks? \\nTYPOGRAPHY & CA: Once a week \\nINFORMATION TECHNOLOGY: Once a week \\nc. Whether the follow up is being done? Yes \\nIV Class Room Assignment daily \\nV Checking and follow-up of assignments done? Yes \\nVI Identification of Talented/Weaker in teachers Diary Yes'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='a. Steps taken to improve Talented students: Promoting them to participate in \\nextracurricular activities, providing them high skill thinking questions for practice. \\nb. Steps taken to improve weaker students : # Peer mentoring and Extra classes \\n# Answers of questions in small points to make \\nlearning easy \\n# Self Practice \\n# By making helping groups \\nVII Steps were taken to improve the truancy in the classes if any: NA \\n2. Vocational Lab Details \\na. All equipment of the lab functional Yes \\nb. If not maintained give the reason \\nc. Stock Register \\ni. Consumable stock register Yes \\nii. Non-Consumable stock register Yes \\niii. Stock validation status Yes \\niv. Mention date of stock verification 30.03.2025 \\nd. Condemnation status \\nIf Yes give details: -- \\nIf No, give reasons: All articles are in good condition \\n3. MAINTENANCE OF RECORDS: \\na. Class Attendance Register Yes \\nb. Maintenance of Teacher’s Diary Yes \\nc. Preparation of Report-Books, Result Sheets : Yes'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='3. MAINTENANCE OF RECORDS: \\na. Class Attendance Register Yes \\nb. Maintenance of Teacher’s Diary Yes \\nc. Preparation of Report-Books, Result Sheets : Yes \\nd. Any other record maintained: Internship report, guest lecture and Industrial \\nvisit record \\n4. Vocational Activities : \\na. Co-Curricular activities undertaken related to Vocational education : Group \\ndiscussions, career guidance \\nb. OJT conducted Yes \\nc. Number of students for whom OJT conducted 42 (class XI) in January, 2025 and 25 \\n  25  (class XII) in May and June, 2025 \\nd. Number of hours of Internship conducted 80 hours above \\ne. Number of Guest lectures conducted, if any Yes, 8 guest lectures \\nf. Specific achievements, if any \\nName and Id of PTVT : LAKSHMI MATANI (200100015) \\nDate : 03/11/2025 \\n \\nSignature of the PTVT'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='PART – B \\n(Assessment by the Head of the School) \\nName of School & ID: GGSSS AZADPUR COLONY, AZADPUR-1309127 \\nName of PTVT and ID: LAKSHMI MATANI (200100015) \\nPeriod of Supervision: \\nThe period of Supervision is less than 3 Months \\n1. Please comment on Part A as filled in by the official and specifically state whether you agree \\nwith the answer or not? If not, give reasons thereof. --- AGREED \\n2. Punctuality (in attending the school as well as class periods, is the PTVT following the \\nprescribed school timings) --- REGULARLY ATTENDED AND PUNCTUAL. \\n3. Whether PTVT attend school regularly on all working days? --- YES \\n4. Number of Leaves taken in that academic year --- 07 CL \\n5. Number of absence in that academic year --- NIL \\n6. Honesty and integrity --- BEYOND DOUBT \\n7. Taken Initiative --- Motivate the students, period wise attendance, stop the truancy \\n8. Whether responsible for any outstanding work during the year? If Yes, give details. --- No'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='7. Taken Initiative --- Motivate the students, period wise attendance, stop the truancy \\n8. Whether responsible for any outstanding work during the year? If Yes, give details. --- No \\n9. Whether reprimanded verbally/in writing for indifferent work or other cause during the year, \\ngive details --- No \\n10. Observation of the Head of the school regarding the teacher’s work and conduct. --- Good and \\nHardworking \\n11. Grading: Outstanding/Very Good/Good/Average/Poor (In case of outstanding/poor give full \\njustification) --- Very Good \\n12. No. and types of charges held – RTI, Vocational consumable and non- consumable stock \\nregister, Co-incharge of ICT lab. \\n \\n \\n \\n \\nDate: Signature of the HOS with date and stamp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:48:13+05:30', 'author': 'COMPUTER LAB PC2', 'moddate': '2025-11-02T19:48:13+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\APAR 2024-25.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'APAR 2024-25.pdf', 'file_type': 'pdf'}, page_content='PART – C \\n(Assessment by the DDE, Zone) \\nName of PTVT and ID: LAKSHMI MATANI (200100015) \\nPeriod of Supervision: \\nComments of DDE Zone \\n \\n \\n \\n \\nDate: Signature and stamp of the DDE Zone'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='MOHIT TIWARI \\nCLOUD COMPUTING AND SECURITY \\nDraw a diagram showing each SDLC phase and where security should be integrated. \\nVaibhav Sharma Naman Arora Hargun Singh Ambar \\n(04511502722) (04611502722) (04711502722) (05111502722) \\nMs. Amandeep Kaur \\n \\n \\n \\n \\nPROJECT BASED LEARNING \\nSubmitted in partial fulfillment of the requirements for the award of the degree of \\nBACHELOR OF TECHNOLOGY \\nin COMPUTER SCIENCE & ENGINEERING \\nBy \\nNaman Arora \\n(04511502722) \\non \\nFake Currency Detection \\n \\n \\nGuided by \\n \\nDr. Preeti Nagrath \\n \\n \\n \\n \\n \\n \\n \\n \\nDEPARTMENT OF COMPUTER SCIENCE & ENGINEERING \\nBHARATI VIDYAPEETH’S COLLEGE OF ENGINEERING \\n(AFFILIATED TO GURU GOBIND SINGH INDRAPRASTHA UNIVERSITY, DELHI) \\nDELHI – 110063 \\nSupervised Deep Learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content=\"Fake Currency Detection \\n \\n1. Overview  \\n \\nThe demand for robust and reliable security systems has driven the rapid adoption of biometrics, \\nwhich utilizes unique physiological or behavioral traits for automatic recognition. Among the \\nvarious biometric modalities, fake c urrency remains the most widely accepted and deployed \\ntechnology globally. Fingerprints —the patterns of ridges and valleys on the surface of the \\nfingertip—are considered the gold standard due to their two fundamental properties: \\n \\n1. Uniqueness: No two individuals, even identical twins, have the same fingerprint pattern. \\n2. Permanence: The patterns are fully formed during fetal development and remain \\nunchanged throughout a person's life, barring severe scarring or damage. \\n \\n2. Problem Statement: The Need for Automation \\n \\nTraditional methods of fingerprint analysis were manual, time -consuming, and prone to human\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='2. Problem Statement: The Need for Automation \\n \\nTraditional methods of fingerprint analysis were manual, time -consuming, and prone to human \\nerror, often relying on trained experts to categorize and match prints. With the massive growth of \\nlarge-scale databases (like those used in law enforcement and national IDs), efficient automation \\nbecame critical. This project addresses the challenge of creating an automated system capable of \\nboth: \\n \\n• Classification: Grouping fingerprints into primary categories (e.g., Arch, Loop, Whorl) \\nto reduce the search space for efficient indexing. \\n• Identification: Pinpointing a specific individual from a database by accurately matching \\ntheir unique fingerprint pattern. \\n \\n3. Project Objective and Methodology \\n \\nThis project aims to leverage advanced computational techniques to create a high-accuracy, end- \\nto-end fingerprint recognition system. \\n \\nObjective Methodology \\n \\nFeature \\nExtraction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='This project aims to leverage advanced computational techniques to create a high-accuracy, end- \\nto-end fingerprint recognition system. \\n \\nObjective Methodology \\n \\nFeature \\nExtraction \\nEmploying Deep Learning , specifically a Convolutional Neural Network (CNN) . \\nUnlike traditional  approaches that manually extract features like  minutiae (ridge \\nendings and bifurcations), the CNN automatically learns the most descriptive features \\ndirectly from the raw image data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='Objective Methodology \\nClassification Implementing a multi-class classification model to categorize fingerprints into the five \\ncommon classes of the Henry system, thus improving database search efficiency. \\n \\nWorking \\nModel \\nDeveloping a robust, working Python application using TensorFlow/Keras for model \\nbuilding and OpenCV for image preprocessing, demonstrating a modern approach to \\nbiometric security. \\n4. Machine Learning & Deep Learning Context \\n \\nThe inherent complexity, noise, and intra-class variation (e.g., rotation, skin distortion, pressure \\ndifferences) in fingerprint images make them challenging for traditional algorithms. Deep \\nLearning (DL) offers a powerful solution: \\n \\n• Machine Learning (ML) Foundation: Historically, ML models like Support Vector \\nMachines (SVMs) and Random Forests were used, but they required labor-intensive \\npreprocessing and feature engineering. \\n• Deep Learning Advantage: CNNs overcome this limitation by utilizing multiple'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='preprocessing and feature engineering. \\n• Deep Learning Advantage: CNNs overcome this limitation by utilizing multiple \\nprocessing layers to learn hierarchical representations—from basic edges and textures \\nin early layers to complex global and local ridge structures in deeper layers. This ability \\nto learn optimal features makes the CNN the most effective tool for pattern recognition \\ntasks like fingerprint analysis. \\n \\n \\nModel Specification: CNN Architecture \\n \\nThe model uses a sequential CNN architecture, a standard and effective design for image \\nclassification tasks. The goal is to build a hierarchy of feature extractors that can capture both \\nlocal patterns (minutiae) and global patterns (ridge flow). \\n \\n1. Model Summary \\nLayer Type Output \\nShape Parameters Purpose \\nInput Layer (96, 96, 1) 0 Defines the input size: 96x96 pixels, 1 channel \\n(grayscale). \\nConv2D (32 filters) (94, 94, 32) 320 Learns basic features (edges, lines).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='Layer Type Output \\nShape Parameters Purpose \\nMaxPooling2D (47, 47, 32) 0 Reduces dimensionality, retains most important features. \\nConv2D (64 filters) (45, 45, 64) 18,496 Learns more complex, localized features (e.g., minutiae). \\nMaxPooling2D (22, 22, 64) 0 Further reduction of feature map size. \\nConv2D \\n(128 filters) (20, 20, 128) 73,856 Learns highly abstract features (e.g., core and delta \\nstructure). \\nMaxPooling2D (10, 10, 128) 0 Final pooling before flattening. \\nFlatten (12,800) 0 Converts 3D feature maps into a 1D vector for the Dense \\nlayers. \\nDense (512 units) (512) 6,554,112 The main feature vector; acts as the fingerprint \\nembedding. \\nDropout (0.5) (512) 0 Prevents overfitting by randomly setting 50% of inputs to \\nzero. \\nDense (5 units) (5) 2,565 Output layer for the 5 classes (Arch, Loop, Whorl, etc.). \\n2. Key Components \\n \\n• Activation Function: ReLU (Rectified Linear Unit) is used in all hidden layers to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content=\"zero. \\nDense (5 units) (5) 2,565 Output layer for the 5 classes (Arch, Loop, Whorl, etc.). \\n2. Key Components \\n \\n• Activation Function: ReLU (Rectified Linear Unit) is used in all hidden layers to \\nintroduce non-linearity, which allows the model to learn complex relationships. \\n• Loss Function: Categorical Cross-Entropy is used because this is a multi-class \\nclassification problem with 5 mutually exclusive classes. \\n• Optimizer: Adam Optimizer is selected for its efficiency and good performance across \\nvarious deep learning tasks. \\n• Metrics: Accuracy is the primary metric used to evaluate the model's performance. \\n \\nTraining Dataset Description \\n \\nThe accuracy and robustness of any fingerprint recognition system heavily depend on the quality \\nand size of the training data.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='1. Typical Datasets (e.g., NIST SD4) \\n \\nReal-world academic projects rely on publicly available standard benchmarks: \\n \\nDataset Type Purpose in Project \\nNIST Special Database 4 \\n(NIST SD4) Public Domain Commonly used for fingerprint classification research. \\nContains roughly 2,000 pairs of fingerprints. \\nFVC (Fingerprint \\nVerification \\nCompetition) \\nCompetition \\nData \\nUsed for testing verification and identification algorithms. \\nFeatures multiple sensors and noise variations. \\n2. Data Characteristics \\nCharacteristic Description Importance for CNN \\n \\nImage Type \\n \\nGrayscale, usually 8-bit. \\nReduces computational complexity \\ncompared to color images. The relevant \\ninformation (ridges/valleys) is intensity- \\nbased. \\n \\nResolution \\nVaries (often 500 DPI). The images are \\nresized to a uniform 96x96 pixels for \\nmodel input. \\nA consistent input size is mandatory for the \\nCNN architecture. \\n \\nClass \\nDistribution \\nImbalanced. Typically, Loop (Left/Right)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content=\"resized to a uniform 96x96 pixels for \\nmodel input. \\nA consistent input size is mandatory for the \\nCNN architecture. \\n \\nClass \\nDistribution \\nImbalanced. Typically, Loop (Left/Right) \\npatterns are the most common, while Arch \\nand Tented Arch are the least common. \\nRequires careful handling (e.g., weighted \\nloss functions or oversampling) to prevent \\nthe model from becoming biased toward the \\nmore frequent classes. \\n \\nVariations \\n(Noise) \\nContains noise from real-world capture, \\nincluding: rotational shift, translation, \\npressure distortion, cuts, and \\ndryness/wetness of the finger. \\nThe CNN's convolutional layers and the use \\nof data augmentation help the model become \\nrobust to these variations. \\n3. Data Preprocessing and Augmentation \\n \\nTo ensure the CNN learns generalized features rather than memorizing training examples, the \\nfollowing steps are performed: \\n \\n• Rescaling (Normalization): All pixel values are scaled from the original range (0-255)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='following steps are performed: \\n \\n• Rescaling (Normalization): All pixel values are scaled from the original range (0-255) \\nto a floating-point range of 0.0 to 1.0. This is a standard practice to improve gradient \\ndescent convergence during training.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='• Data Augmentation: The ImageDataGenerator applies geometric transformations to the \\ntraining images on-the-fly, creating synthetic variations. This includes: \\no Rotation \\no Zooming \\no Shifting (width/height) \\no Horizontal Flips \\nThis augmentation is crucial for building a robust identification system that can handle \\ndifferences in how a user places their finger on a scanner. \\n \\nFor Classification (Grouping by Pattern Type) \\nThe primary standard for the classification task (Arch, Loop, Whorl) is the NIST Special \\nDatabase 4 (NIST SD4). \\n \\n• Key Features: \\no Purpose: Specifically designed for research on automated fingerprint \\nclassification. \\no Labels: Contains approximately 2,000 pairs of fingerprints (4,000 images total) \\nthat are pre-labeled according to the Henry classification system (Arch, Tented \\nArch, Right Loop, Left Loop, Whorl). \\no Utility: Its clear, predefined classes make it the ideal benchmark for training a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='Arch, Right Loop, Left Loop, Whorl). \\no Utility: Its clear, predefined classes make it the ideal benchmark for training a \\nCNN to categorize the global ridge structure of a fingerprint. \\n \\nFor Identification (Matching to an Individual) \\nFor the identification and verification tasks, datasets that provide multiple, varied impressions \\nof the same finger are crucial. The Fingerprint Verification Competition (FVC) Series is the \\nindustry standard. \\n \\n• Key Features: \\no Purpose: Designed for testing the robustness of matching and identification \\nalgorithms. \\no Multiple Impressions: Provides multiple impressions (usually 8-12) for each \\nunique finger, captured under varying conditions (rotation, pressure, noise, skin \\ndistortion, different time gaps). \\no Sub-Databases: The FVC series (FVC2000, FVC2002, FVC2004, etc.) are \\nseparated into categories based on the sensor type used (e.g., optical sensor, \\ncapacitive sensor, synthetic generator), allowing you to test generalization across'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='separated into categories based on the sensor type used (e.g., optical sensor, \\ncapacitive sensor, synthetic generator), allowing you to test generalization across \\nhardware. \\no Utility: Training on FVC data forces the deep learning model to learn a highly \\nstable feature representation (embedding) that is invariant to common real-world \\nnoise and distortion, which is essential for accurate individual identification.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='Code: \\ndef identify_individual(new_image_path, base_cnn_model, training_image_paths, training_embeddings): \\nif not training_image_paths or training_embeddings is None or len(training_image_paths) != \\ntraining_embeddings.shape[0]: \\nprint(\"Error: Training data is not properly loaded or provided.\") return None, None \\ntry: \\nnew_image = load_and_preprocess_image(new_image_path, target_size=(IMG_SIZE, IMG_SIZE)) \\nnew_image = np.expand_dims(new_image, axis=0) except Exception as e: \\nprint(f\"Error preprocessing new image: {e}\") return None, None \\n try: \\nnew_embedding = base_cnn_model.predict(new_image) except Exception \\nas e: \\nprint(f\"Error getting embedding for new image: {e}\") return None, None \\ndistances = np.sum(np.abs(new_embedding - training_embeddings), axis=1) \\n \\nmin_distance_index = np.argmin(distances) \\nmin_distance = distances[min_distance_index] closest_training_image_path = \\ntraining_image_paths[min_distance_index]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='min_distance_index = np.argmin(distances) \\nmin_distance = distances[min_distance_index] closest_training_image_path = \\ntraining_image_paths[min_distance_index] \\nidentified_individual = os.path.basename(closest_training_image_path).split(\\'_\\')[0] \\nreturn identified_individual, min_distance \\n \\nbase_cnn_model_for_identification = siamese_model.get_layer(\\'sequential_1\\') # \\ndata_dir = os.path.join(base_path, \\'dataset_FVC2000_DB4_B\\', \\'dataset\\', \\'train_data\\') \\nall_image_paths_for_embedding = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\\'.bmp\\')] \\ntraining_image_paths_for_embedding = train_paths # Use the paths from the train_test_split \\ntraining_embeddings = [] \\nprint(\"Generating embeddings for training images...\") for img_path in \\ntraining_image_paths_for_embedding: \\ntry: \\nimg = load_and_preprocess_image(img_path, target_size=(IMG_SIZE, IMG_SIZE)) \\nimg = np.expand_dims(img, axis=0) embedding = \\nbase_cnn_model_for_identification.predict(img)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='try: \\nimg = load_and_preprocess_image(img_path, target_size=(IMG_SIZE, IMG_SIZE)) \\nimg = np.expand_dims(img, axis=0) embedding = \\nbase_cnn_model_for_identification.predict(img) \\ntraining_embeddings.append(embedding[0])  \\nexcept Exception as e: \\nprint(f\"Error generating embedding for {img_path}: {e}\") \\n \\n \\n \\ntraining_embeddings = np.array(training_embeddings) \\nprint(f\"Generated embeddings for {training_embeddings.shape[0]} training images.\") \\nsample_new_image_path = test_paths[0] \\nsample_true_individual = os.path.basename(sample_new_image_path).split(\\'_\\')[0] print(f\"\\\\nIdentifying individual for \\nsample image: {sample_new_image_path}\") \\nidentified_individual, min_distance = identify_individual( sample_new_image_path, \\nbase_cnn_model_for_identification, training_image_paths_for_embedding, \\ntraining_embeddings \\n) \\nif identified_individual is not None: \\nprint(f\"Identified Individual: {identified_individual}\") print(f\"Minimum Distance to closest training'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='training_embeddings \\n) \\nif identified_individual is not None: \\nprint(f\"Identified Individual: {identified_individual}\") print(f\"Minimum Distance to closest training \\nimage: {min_distance:.4f}\") print(f\"True Individual (for this sample): {sample_true_individual}\")'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='similarity_threshold = 0.5  \\nif min_distance < similarity_threshold: \\nprint(\"Match is considered positive (below similarity threshold).\") else: \\nprint(\"Match is considered negative (above similarity threshold).\") \\nOutput: \\n \\nResult: \\nNIST DB4 (3-class classification): \\n \\n• Dataset Size: 1679 training images, 479 validation images, and 243 testing images across 3 classes. \\n• Model: A standard CNN model. \\n• Training Performance: The model trained for 10 epochs showed increasing accuracy on the training data. \\n• Validation Performance: The validation accuracy also improved over epochs, reaching approximately 86.01%. \\n• Test Performance: The model achieved a test loss of approximately 0.4624 and a test accuracy of \\napproximately 0.8272 on the unseen test set. \\n \\nFVC2000 DB4-B (Individual Identification - Siamese Network): \\n \\n• Dataset Size: The original dataset was split into 560 training images, 120 validation images, and 120 testing \\nimages for creating pairs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='• Dataset Size: The original dataset was split into 560 training images, 120 validation images, and 120 testing \\nimages for creating pairs. \\n• Model: A Siamese network with a shared base CNN. \\n• Training Performance: The Siamese model trained for 10 epochs showed increasing accuracy on the training \\npairs. \\n• Validation Performance: The validation accuracy fluctuated and reached approximately 73.21%. \\n• Test Performance: The model achieved a test loss of approximately 1.0447 and a test accuracy of \\napproximately 0.6518 on the unseen test pairs. \\n \\nSummary Comparison: \\nThe CNN model trained for 3 -class classification on the NIST DB4 dataset achieved a higher test accuracy (around 83%) \\ncompared to the Siamese network trained for individual identification on the FVC2000 DB4 -B dataset (around 65%). This \\nsuggests that classifying the general pattern of a fingerprint might be an easier task than identifying a specific individual'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-04T08:36:17+05:30', 'author': 'Naman Arora', 'moddate': '2025-11-04T08:36:17+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\sdl pbl naman.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'sdl pbl naman.pdf', 'file_type': 'pdf'}, page_content='suggests that classifying the general pattern of a fingerprint might be an easier task than identifying a specific individual \\nfrom their fingerprint with the current Siamese network architecture and training on this specific dataset split.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:50:45+05:30', 'author': 'Lenovo', 'moddate': '2025-11-02T19:50:45+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\WORK AND CONDUCT REPORT.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'WORK AND CONDUCT REPORT.pdf', 'file_type': 'pdf'}, page_content='WORK AND CONDUCT REPORT FOR RELEASE OF SALARY AND CONTINUATION OF POST \\nName in full (in Block Letters) LAKSHMI MATANI \\nEmployee ID 200100015 \\nQualified/Non-Qualified Qualified \\nSchool Name and ID GOVT. GIRLS SR. SEC. SCHOOL, AZADPUR COLONY, \\nAZADPUR   \\nSCHOOL ID - 1309127 \\nZone 09 \\nFather’s/Husband’s Name LALIT ARORA \\nSubject (as per appointment letter) STENOGRAPHY (ENGLISH) \\nDate of Birth 02/07/1975 \\nDate of Initial Appointment 03/02/2001 \\nDate of Posting in present School 27/04/2023 \\nContact No. 9911657744 \\nWORK ALLOCATION \\nAllotted \\nSchool \\nTrade/Subject \\npresently \\nteaching \\nNo of \\nstudents \\nenrolled in \\nAY 24-25 \\nTotal periods allotted \\nin a week for teaching \\nNSQF/Skill subjects \\nonly \\nTotal periods \\nallotted in a week \\nfor subjects other \\nthan skill subject  \\nTotal Periods \\nAllotted per \\nweek \\nIX INFORMATION \\nTECHNOLOGY \\n158 12 NIL 12 \\nX INFORMATION \\nTECHNOLOGY \\n121 12 NIL 12 \\nXI TYPOGRAPHY & \\nCOMP. APP. \\n42 6 NIL 6 \\nXII TYPOGRAPHY & \\nCOMP. APP. \\n51 6 NIL 6'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:50:45+05:30', 'author': 'Lenovo', 'moddate': '2025-11-02T19:50:45+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\WORK AND CONDUCT REPORT.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'WORK AND CONDUCT REPORT.pdf', 'file_type': 'pdf'}, page_content='Allotted per \\nweek \\nIX INFORMATION \\nTECHNOLOGY \\n158 12 NIL 12 \\nX INFORMATION \\nTECHNOLOGY \\n121 12 NIL 12 \\nXI TYPOGRAPHY & \\nCOMP. APP. \\n42 6 NIL 6 \\nXII TYPOGRAPHY & \\nCOMP. APP. \\n51 6 NIL 6 \\nDETAILS OF RESULT (AY 2024-2025) \\nClass No of Students appeared for \\nexam (AY 2024-2025) \\nNo of students passed in \\nexam \\nResult of 2024-25(in %) \\nIX 156 156 100% \\nX 121 121 100% \\nXI 42 42 100% \\nXII 51 51 100% \\nDetails of Training attended in 2024-2025 & 2025-2026 \\nS.No Finish complete details of Training undertaken by PTVT \\n1 Continuous Professional Development for 3 days at DIET, Keshav Puram (24/10/2024 to \\n26/10/2024) \\n2 Continuous Professional Development for 5 days at RPVV, Sec-5, Dwarka (School id – 1821286) \\n(22/09/2025 to 26/09/2025) \\n  \\n Annual work report for the session 2024-25 has been completed:  Yes \\n Grading of Annual work report for the session (2024-25):               (Copy of Annual work report is enclosed for \\ninformation)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-02T19:50:45+05:30', 'author': 'Lenovo', 'moddate': '2025-11-02T19:50:45+05:30', 'source': '..\\\\data\\\\text_files\\\\pdf\\\\WORK AND CONDUCT REPORT.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'WORK AND CONDUCT REPORT.pdf', 'file_type': 'pdf'}, page_content='Any Special achievement other than result (AY 2024-2025) \\n \\nAssessment By HOS \\nCertified that his/her behavior with HOS, Staff members & students is cordial. \\n*If answer is in yes provide supporting documents with comment of HOS \\n \\n \\nSignature of HOS \\n1. Work conduct report of PTVT Satisfied \\n2. Integrity of the PTVT Beyond doubt \\n3. Any Memorandum issued to PTVT for non-compliance of direction of HOS No \\n4. Whether there is any Complain against PTVT in School No')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b6241",
   "metadata": {},
   "source": [
    "### embedding and vectorStore Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdbfcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List , Dict , Any , Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed1820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x211abce7390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated Embedding with shape  : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "## initialization the embedding manager\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb69f45",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec7c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x211abce7fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name:str=\"pdf_documents\",persist_directory:str=\"../data/vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embedding for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77d1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 33 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (33, 384)\n",
      "Adding 33 documents to vector store...\n",
      "Successfully added 33 documents to vector store\n",
      "Total documents in collection: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33df6f3",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e2eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7c15e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2692c2eb290>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b917c1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'naman'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_retriever.retrieve(\"naman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a953343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98d34ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c90ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a645b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c5439",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31536484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc7610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7481af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.1-8b-instant\", api_key: str =None):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b79479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: llama-3.1-8b-instant\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34d5d1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6962a14",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5964f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cdb86ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "No relevant context found to answer the question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592a61b",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipelines Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "319a0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1df4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding with shape  : (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "\n",
      "Final Answer: No relevant context found.\n",
      "Summary: There is no information provided to summarize.\n",
      "History: {'question': 'what is attention is all you need', 'answer': 'No relevant context found.', 'sources': [], 'summary': 'There is no information provided to summarize.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
